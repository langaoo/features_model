"""tools/test_dp_rgb_pipeline.py

æµ‹è¯• DP RGB è®­ç»ƒç®¡é“çš„å„ä¸ªç»„ä»¶ã€‚

ç”¨æ³•:
    python tools/test_dp_rgb_pipeline.py --config configs/train_dp_rgb_default.yaml
"""

from __future__ import annotations

import argparse
import sys
from pathlib import Path

import torch
import yaml

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from features_common.dp_rgb_dataset import DPRGBDataset, collate_fn
from features_common.dp_rgb_policy import DiffusionRGBPolicy
from torch.utils.data import DataLoader


def test_dataset(config: dict):
    """æµ‹è¯•æ•°æ®é›†"""
    print("\n" + "="*80)
    print("æµ‹è¯• 1: æ•°æ®é›†")
    print("="*80)
    
    try:
        dataset = DPRGBDataset(
            rgb_zarr_roots=config['rgb_zarr_roots'],
            traj_root=config['traj_root'],
            tasks=config['tasks'],
            horizon=config['horizon'],
            n_obs_steps=config['n_obs_steps'],
            use_left_arm=config.get('use_left_arm', True),
            use_right_arm=config.get('use_right_arm', False),
            fuse_arms=config.get('fuse_arms', False),
        )
        
        print(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸ: {len(dataset)} ä¸ªæ ·æœ¬")
        
        # æµ‹è¯•é‡‡æ ·
        print("\næµ‹è¯•é‡‡æ ·...")
        sample = dataset[0]
        print(f"  Task: {sample.task}")
        print(f"  Episode: {sample.episode}")
        print(f"  Start idx: {sample.start_idx}")
        print(f"  Obs shape: {sample.obs.shape}")  # [To, C]
        print(f"  Action shape: {sample.action.shape}")  # [Ta, A]
        
        # æµ‹è¯• DataLoader
        print("\næµ‹è¯• DataLoader...")
        loader = DataLoader(
            dataset,
            batch_size=2,
            shuffle=False,
            num_workers=0,
            collate_fn=collate_fn,
        )
        
        batch = next(iter(loader))
        print(f"  Batch obs shape: {batch['obs'].shape}")  # [B, To, C]
        print(f"  Batch action shape: {batch['action'].shape}")  # [B, Ta, A]
        
        print("\nâœ… æ•°æ®é›†æµ‹è¯•é€šè¿‡!")
        return True, dataset, sample
    
    except Exception as e:
        print(f"\nâŒ æ•°æ®é›†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None, None


def test_policy(config: dict, sample):
    """æµ‹è¯• policy"""
    print("\n" + "="*80)
    print("æµ‹è¯• 2: Policy")
    print("="*80)
    
    try:
        obs_dim = sample.obs.shape[-1]
        action_dim = sample.action.shape[-1]
        
        print(f"  Obs dim: {obs_dim}")
        print(f"  Action dim: {action_dim}")
        
        # åˆ›å»º policy
        policy = DiffusionRGBPolicy(
            obs_dim=obs_dim,
            action_dim=action_dim,
            horizon=config['horizon'],
            n_obs_steps=config['n_obs_steps'],
            n_action_steps=config['n_action_steps'],
            rgb_ckpt_path=config.get('rgb_ckpt_path'),
            freeze_encoder=config.get('freeze_encoder', False),
            obs_encoder_dim=config.get('obs_encoder_dim', 256),
            obs_as_global_cond=config.get('obs_as_global_cond', True),
        )
        
        print(f"\nâœ… Policy åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
        device = torch.device(config.get('device', 'cpu'))
        policy = policy.to(device)
        policy.eval()
        
        # å‡†å¤‡è¾“å…¥
        obs = sample.obs.unsqueeze(0).to(device)  # [1, To, C]
        action = sample.action.unsqueeze(0).to(device)  # [1, Ta, A]
        
        batch = {
            'obs': obs,
            'action': action,
        }
        
        # æµ‹è¯• compute_loss
        print("  æµ‹è¯• compute_loss...")
        with torch.no_grad():
            loss = policy.compute_loss(batch)
        print(f"    Loss: {loss.item():.4f}")
        
        # æµ‹è¯• predict_action
        print("  æµ‹è¯• predict_action...")
        with torch.no_grad():
            result = policy.predict_action({'obs': obs})
        
        print(f"    Action shape: {result['action'].shape}")  # [1, Ta, A]
        print(f"    Action pred shape: {result['action_pred'].shape}")  # [1, horizon, A]
        
        print("\nâœ… Policy æµ‹è¯•é€šè¿‡!")
        return True, policy
    
    except Exception as e:
        print(f"\nâŒ Policy æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None


def test_training_step(config: dict, dataset, policy):
    """æµ‹è¯•è®­ç»ƒæ­¥éª¤"""
    print("\n" + "="*80)
    print("æµ‹è¯• 3: è®­ç»ƒæ­¥éª¤")
    print("="*80)
    
    try:
        device = torch.device(config.get('device', 'cpu'))
        policy = policy.to(device)
        policy.train()
        
        # åˆ›å»º optimizer
        optimizer = torch.optim.AdamW(
            policy.parameters(),
            lr=config.get('lr', 1e-4),
        )
        
        # åˆ›å»º DataLoader
        loader = DataLoader(
            dataset,
            batch_size=2,
            shuffle=False,
            num_workers=0,
            collate_fn=collate_fn,
        )
        
        # è®­ç»ƒä¸€ä¸ª batch
        batch = next(iter(loader))
        batch = {k: v.to(device) for k, v in batch.items()}
        
        print("  è®­ç»ƒä¸€ä¸ª batch...")
        optimizer.zero_grad()
        loss = policy.compute_loss(batch)
        loss.backward()
        optimizer.step()
        
        print(f"    Loss: {loss.item():.4f}")
        print(f"    æ¢¯åº¦èŒƒæ•°: {sum(p.grad.norm().item() for p in policy.parameters() if p.grad is not None):.4f}")
        
        print("\nâœ… è®­ç»ƒæ­¥éª¤æµ‹è¯•é€šè¿‡!")
        return True
    
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒæ­¥éª¤æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", type=str, required=True, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    args = ap.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    print("="*80)
    print("DP RGB ç®¡é“æµ‹è¯•")
    print("="*80)
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"ä»»åŠ¡: {config.get('tasks', [])}")
    print(f"RGB ckpt: {config.get('rgb_ckpt_path', 'None')}")
    
    # æµ‹è¯•æ•°æ®é›†
    success, dataset, sample = test_dataset(config)
    if not success:
        print("\nâŒ æµ‹è¯•å¤±è´¥: æ•°æ®é›†")
        return
    
    # æµ‹è¯• policy
    success, policy = test_policy(config, sample)
    if not success:
        print("\nâŒ æµ‹è¯•å¤±è´¥: Policy")
        return
    
    # æµ‹è¯•è®­ç»ƒæ­¥éª¤
    success = test_training_step(config, dataset, policy)
    if not success:
        print("\nâŒ æµ‹è¯•å¤±è´¥: è®­ç»ƒæ­¥éª¤")
        return
    
    print("\n" + "="*80)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("="*80)
    print("\nä½ å¯ä»¥å¼€å§‹è®­ç»ƒäº†:")
    print(f"  python tools/train_dp_rgb.py --config {args.config}")


if __name__ == "__main__":
    main()
