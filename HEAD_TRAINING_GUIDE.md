# Headè®­ç»ƒå®Œæ•´æµç¨‹æŒ‡å—

ç”Ÿæˆæ—¶é—´: 2026å¹´1æœˆ10æ—¥

---

## ğŸ“Š é—®é¢˜1: Headé˜¶æ®µçš„æ•°æ®è¾“å…¥

### âœ… æ•°æ®æµç¨‹å›¾

```
è¾“å…¥æ•°æ®æº:
â”œâ”€ 4æ¨¡å‹zarrç‰¹å¾ (rgb_dataset/features_*_zarr/)
â”‚   â”œâ”€ features_croco_encoder_dict_unified_zarr/
â”‚   â”œâ”€ features_vggt_encoder_dict_unified_zarr/
â”‚   â”œâ”€ features_dinov3_encoder_dict_unified_zarr/
â”‚   â””â”€ features_da3_encoder_dict_unified_zarr/
â”‚
â””â”€ Trajectoryæ•°æ® (raw_data/)
    â””â”€ <task>/demo_randomized/_traj_data/episode*.pkl

å¤„ç†æµç¨‹:
1. DPRGB4ModelDatasetè¯»å–zarr â†’ obs[To, 4, 2048]
2. DPRGB4ModelDatasetè§£æpkl â†’ action[Ta, A]
3. RGB2PCAlignedEncoder(å†»ç»“) â†’ obs_feat[To, 1280]
4. DiffusionRGBHead â†’ é¢„æµ‹action
```

### ğŸ¯ æ•°æ®æ ¼å¼è¯´æ˜

#### Zarrç‰¹å¾æ ¼å¼
```
features_croco_encoder_dict_unified_zarr/
â””â”€â”€ beat_block_hammer-demo_randomized-20_sapien_head_camera/
    â”œâ”€â”€ episode_0.zarr/
    â”‚   â”œâ”€â”€ per_frame_features  # zarr array [W, T=8, Hf, Wf, C=1024]
    â”‚   â”œâ”€â”€ frame_paths.json
    â”‚   â””â”€â”€ meta.json
    â”œâ”€â”€ episode_1.zarr/
    â””â”€â”€ ...
```

#### Trajectoryæ ¼å¼
```python
# episode0.pklå†…å®¹:
{
    'left_joint_path': [
        {
            'status': 'success',
            'position': np.array([N, 6], dtype=float32),  # 6Då…³èŠ‚è§’åº¦
            'velocity': np.array([N, 6], dtype=float32),
        },
        ...  # 5ä¸ªwaypoints
    ],
    'right_joint_path': [...]  # åŒå·¦è‡‚
}
```

### ğŸ’» ä½¿ç”¨ä½ çš„æ•°æ®è®­ç»ƒ

```bash
# 1. æµ‹è¯•æ•°æ®pipeline
python tools/test_head_training_pipeline.py \
  --task beat_block_hammer-demo_randomized-20_sapien_head_camera \
  --traj_root /home/gl/features_model/raw_data \
  --encoder_ckpt outputs/train_rgb2pc_runs/run_best_bs32/ckpt_step_0002000.pt

# 2. è¿è¡Œè®­ç»ƒ (å•è‡‚6D)
python tools/train_dp_rgb_single_task_4models.py \
  --task beat_block_hammer-demo_randomized-20_sapien_head_camera \
  --traj_root /home/gl/features_model/raw_data \
  --encoder_ckpt outputs/train_rgb2pc_runs/run_best_bs32/ckpt_step_0002000.pt \
  --use_left_arm \
  --epochs 50 \
  --batch_size 16 \
  --save_dir outputs/dp_rgb_runs/beat_block_hammer_6d

# 3. è®­ç»ƒåŒè‡‚12D
python tools/train_dp_rgb_single_task_4models.py \
  --task beat_block_hammer-demo_randomized-20_sapien_head_camera \
  --encoder_ckpt outputs/train_rgb2pc_runs/run_best_bs32/ckpt_step_0002000.pt \
  --use_left_arm --use_right_arm --fuse_arms \
  --epochs 50 \
  --save_dir outputs/dp_rgb_runs/beat_block_hammer_12d

# 4. è®­ç»ƒåŒè‡‚+å¤¹çˆª14D
python tools/train_dp_rgb_single_task_4models.py \
  --task beat_block_hammer-demo_randomized-20_sapien_head_camera \
  --encoder_ckpt outputs/train_rgb2pc_runs/run_best_bs32/ckpt_step_0002000.pt \
  --use_left_arm --use_right_arm --fuse_arms --include_gripper \
  --epochs 50 \
  --save_dir outputs/dp_rgb_runs/beat_block_hammer_14d
```

### ğŸ“ æ•°æ®å¤„ç†é€»è¾‘ (å·²å®ç°)

ä½ç½®: `features_common/dp_rgb_dataset_4models.py`

```python
class DPRGB4ModelDataset:
    def __getitem__(self, idx):
        # 1. ä»4ä¸ªzarrè¯»å–ç‰¹å¾
        packs = [load_zarr_pack(root/task/episode.zarr) for root in roots_4]
        
        # 2. å–[Wi, Ti]å¸§ç‰¹å¾,æ¯ä¸ªæ¨¡å‹å¹³å‡æ± åŒ–åˆ°ä¸€ä¸ªå‘é‡
        for pack in packs:
            f = pack.get_frame(wi, ti)  # [Hf, Wf, C]
            f = f.reshape(-1, C).mean(axis=0)  # [C]
        
        # 3. å †å 4ä¸ªæ¨¡å‹ â†’ obs[To, 4, 2048]
        obs = stack_frames_across_models()
        
        # 4. ä»pklè§£æaction
        traj = pickle.load(traj_pkl)
        left_path = traj['left_joint_path']
        action = parse_joint_path(left_path)  # [T, 6]
        
        # 5. åˆ‡ç‰‡actionçª—å£ â†’ action[Ta, A]
        action = action[start:start+horizon]
        
        return obs, action
```

---

## ğŸ“Š é—®é¢˜2: ç¦»çº¿æ¨ç†é˜¶æ®µ

### âœ… æ˜¯çš„!ç¦»çº¿æ¨ç†ä½¿ç”¨ç›¸åŒçš„æ•°æ®å¤„ç†æµç¨‹

```
è®­ç»ƒé˜¶æ®µ:
  zarrç‰¹å¾ + pklè½¨è¿¹ â†’ Dataset â†’ Encoder+Head â†’ é¢„æµ‹action â†’ è®¡ç®—loss

ç¦»çº¿æ¨ç†é˜¶æ®µ:
  zarrç‰¹å¾ (no pkl) â†’ Dataset â†’ Encoder+Head â†’ é¢„æµ‹action â†’ ä¿å­˜/å¯è§†åŒ–
```

### ğŸ’» ç¦»çº¿æ¨ç†ä½¿ç”¨æ–¹æ³•

```bash
# æ–¹æ³•1: ä½¿ç”¨infer_dp_rgb_4models.py
python tools/infer_dp_rgb_4models.py \
  --ckpt outputs/dp_rgb_runs/beat_block_hammer_6d/final_head.pt \
  --task beat_block_hammer-demo_randomized-20_sapien_head_camera \
  --episode episode_0 \
  --exec_steps 50
```

### ğŸ” åˆ¤æ–­æ¨ç†æ˜¯å¦æˆåŠŸ

#### æ–¹æ³•1: æ£€æŸ¥è¾“å‡ºç»´åº¦å’Œæ•°å€¼
```python
# infer_dp_rgb_4models.py ä¼šæ‰“å°:
Predicted actions shape: [T, A]  # Tæ˜¯æ‰§è¡Œæ­¥æ•°, Aæ˜¯åŠ¨ä½œç»´åº¦
Action range: [min, max]
```

#### æ–¹æ³•2: å¯è§†åŒ–actionè½¨è¿¹
```bash
# ä¿å­˜é¢„æµ‹çš„actionå¹¶ä¸ground truthæ¯”è¾ƒ
python - << 'PY'
import torch
import matplotlib.pyplot as plt

# åŠ è½½é¢„æµ‹
pred = torch.load('outputs/predicted_actions.pt')
# åŠ è½½çœŸå®action
import pickle
with open('raw_data/.../episode_0.pkl', 'rb') as f:
    gt = pickle.load(f)['left_joint_path']

# ç»˜åˆ¶å¯¹æ¯”å›¾
plt.plot(pred[:, 0], label='pred joint 0')
plt.plot(gt[0]['position'][:, 0], label='gt joint 0')
plt.legend()
plt.savefig('action_comparison.png')
PY
```

#### æ–¹æ³•3: åœ¨RoBoTwinä¸­æ‰§è¡Œ (é—®é¢˜3ä¼šè¯¦ç»†è®²)

---

## ğŸ“Š é—®é¢˜3: åœ¨çº¿æ¨ç† - é›†æˆåˆ°RoBoTwin

### âš ï¸ é‡è¦åŒºåˆ«

```
ç¦»çº¿æ¨ç†: ä½¿ç”¨é¢„æå–çš„zarrç‰¹å¾
åœ¨çº¿æ¨ç†: ä»ç¯å¢ƒå®æ—¶è·å–RGB â†’ æå–ç‰¹å¾ â†’ é¢„æµ‹action
```

### ğŸ”§ åœ¨RoBoTwinä¸­è¿è¡Œçš„æ­¥éª¤

#### æ­¥éª¤1: ä¿®æ”¹RoBoTwinçš„policyæ¥å£

åœ¨ `RoBoTwin/policy/` ä¸‹åˆ›å»ºä½ çš„policyç±»:

```python
# RoBoTwin/policy/DP_RGB_4Models/dp_rgb_4models_policy.py
import torch
import sys
from pathlib import Path

# æ·»åŠ ä½ çš„é¡¹ç›®è·¯å¾„
FEATURES_MODEL_ROOT = Path('/home/gl/features_model')
sys.path.insert(0, str(FEATURES_MODEL_ROOT))

from features_common.rgb2pc_aligned_encoder_4models import RGB2PCAlignedEncoder4Models
from features_common.dp_rgb_policy_multitask import DiffusionRGBHead

class DPRGB4ModelsPolicy:
    def __init__(self, ckpt_path, device='cuda'):
        # åŠ è½½checkpoint
        ckpt = torch.load(ckpt_path, map_location='cpu')
        
        # åŠ è½½encoder
        encoder_ckpt = ckpt['encoder_ckpt']
        self.encoder = RGB2PCAlignedEncoder4Models.from_checkpoint(
            encoder_ckpt, freeze=True
        ).to(device).eval()
        
        # åŠ è½½head
        self.head = DiffusionRGBHead(...)
        self.head.load_state_dict(ckpt['head_state'])
        self.head.to(device).eval()
        
        # åŠ è½½normalizer
        self.normalizer = ckpt['normalizer']
        
        self.device = device
        self.obs_buffer = deque(maxlen=2)  # n_obs_steps=2
    
    def reset(self):
        self.obs_buffer.clear()
    
    def predict(self, obs_dict):
        """
        Args:
            obs_dict: {
                'head_camera': np.array([H, W, 3]),  # RGBå›¾åƒ
                # æˆ–è€…ç›´æ¥æä¾›ç‰¹å¾:
                'features_4models': np.array([4, C]),  # å¦‚æœå·²ç»æå–
            }
        Returns:
            action: np.array([A])
        """
        # è·å–ç‰¹å¾ (è¿™é‡Œéœ€è¦å®ç°4æ¨¡å‹çš„ç‰¹å¾æå–)
        if 'features_4models' in obs_dict:
            feat = obs_dict['features_4models']
        else:
            # åœ¨çº¿æå–ç‰¹å¾ (éœ€è¦åŠ è½½4ä¸ªbackbone,æ˜¾å­˜éœ€æ±‚å¤§)
            feat = self._extract_features_online(obs_dict['head_camera'])
        
        # æ·»åŠ åˆ°buffer
        self.obs_buffer.append(feat)
        
        # å¦‚æœbufferä¸å¤Ÿ,å¡«å……
        while len(self.obs_buffer) < 2:
            self.obs_buffer.append(feat)
        
        # æ„å»ºobs tensor [To, 4, C]
        obs_seq = np.stack(list(self.obs_buffer), axis=0)
        obs_t = torch.from_numpy(obs_seq).unsqueeze(0).float().to(self.device)
        
        # Encoder
        with torch.no_grad():
            obs_feat = self.encoder(obs_t)  # [1, To, 1280]
            
            # Headé¢„æµ‹
            action_pred = self.head.predict_action({'obs': obs_feat})
            action = action_pred['action'][0, 0].cpu().numpy()  # å–ç¬¬ä¸€æ­¥
        
        # Denormalize
        action = self.normalizer['action'].unnormalize(
            torch.from_numpy(action).unsqueeze(0)
        ).squeeze(0).numpy()
        
        return action

def get_model(args):
    """RoBoTwinè°ƒç”¨çš„æ¥å£"""
    return DPRGB4ModelsPolicy(
        ckpt_path=args['ckpt_path'],
        device='cuda'
    )
```

#### æ­¥éª¤2: ä¿®æ”¹RoBoTwinçš„evalé…ç½®

```yaml
# RoBoTwin/task_config/beat_block_hammer_dp_rgb.yml
policy_name: "DP_RGB_4Models"
ckpt_path: "/home/gl/features_model/outputs/dp_rgb_runs/beat_block_hammer_6d/final_head.pt"
data_type:
  use_rgbd_pointcloud: false  # ä¸éœ€è¦ç‚¹äº‘
  use_rgb_features: true      # ä½¿ç”¨RGBç‰¹å¾
```

#### æ­¥éª¤3: è¿è¡Œè¯„ä¼°

```bash
cd /home/gl/features_model/RoBoTwin

python script/eval_policy.py \
  --task_name beat_block_hammer \
  --task_config beat_block_hammer_dp_rgb \
  --policy_name DP_RGB_4Models \
  --ckpt_setting final_head \
  --seed 0 \
  --eval_test_num 10
```

### ğŸš€ ç®€åŒ–æ–¹æ¡ˆ: å…ˆç”¨ç¦»çº¿ç‰¹å¾éªŒè¯

```python
# åœ¨RoBoTwinä¸­ä½¿ç”¨é¢„æå–çš„zarrç‰¹å¾(ä¸éœ€è¦åœ¨çº¿æå–)
class DPRGB4ModelsPolicyOffline:
    def __init__(self, ckpt_path, zarr_roots, task, episode):
        # åŠ è½½æ¨¡å‹
        ...
        
        # åŠ è½½zarrç‰¹å¾
        self.features_cache = {}
        for i, root in enumerate(zarr_roots):
            pack = load_zarr_pack(root / task / f"{episode}.zarr")
            self.features_cache[i] = pack
        
        self.step_idx = 0
    
    def predict(self, obs_dict):
        # ç›´æ¥ä»zarrè¯»å–ç‰¹å¾
        wi = self.step_idx // 8
        ti = self.step_idx % 8
        
        feat_4 = []
        for i in range(4):
            f = self.features_cache[i].get_frame(wi, ti)
            f = f.reshape(-1, f.shape[-1]).mean(axis=0)
            feat_4.append(f)
        
        feat_4 = np.stack(feat_4, axis=0)  # [4, C]
        
        # æ·»åŠ åˆ°bufferå¹¶é¢„æµ‹
        ...
        
        self.step_idx += 1
        return action
```

---

## ğŸ“Š é—®é¢˜4: å†—ä½™æ–‡ä»¶æ‰«æ

è®©æˆ‘æ‰«ætools/ä¸‹çš„æ–‡ä»¶...