# To（n_obs_steps）取值建议

## TL;DR

**推荐：To=2（默认） 或 To=1（低延迟）**

| To | 适用场景 | 优点 | 缺点 |
|----|---------|------|------|
| **1** | 快速响应、静态场景 | 最低延迟、最少计算 | 无历史信息，动态任务性能差 |
| **2** | 大多数桌面操作任务 | 简单速度信息，足够泛化 | 延迟可接受 |
| **4** | 动态物体、复杂交互 | 更完整运动轨迹 | 延迟翻倍、可能过拟合 |
| **8+** | 特殊复杂任务 | 完整运动历史 | 延迟高、容易过拟合训练数据 |

---

## 1. To 与对齐训练的 8 帧切片无关

**重要澄清**：对齐模块训练时用 8 帧切片是**数据增强策略**（从长序列里随机采样，增加训练样本多样性），与 DP 推理时的 To **没有强制依赖**。

- 对齐 encoder 的输入 shape: `[B, To, 4, 2048]`，其中 `To` 可以是任意长度（1/2/4/8/...）。
- 训练时用 8 是为了让 encoder 学会处理"连续若干帧的融合"，但推理时你可以用 To=2，encoder 一样能 forward。

---

## 2. To 取值的三个考量维度

### 2.1 任务动态性（Task Dynamics）

- **静态场景（物体不动、目标固定）**：To=1 足够，因为当前帧已包含全部状态。
  - 例子：`pick_and_place` 固定物体、`button_press` 固定目标。

- **动态场景（物体运动、需要预测轨迹）**：To≥2，需要速度/加速度信息。
  - 例子：`shake_bottle`、`beat_block_hammer`（物体在晃动）、双臂协同（需要感知两臂相对运动）。

### 2.2 控制频率与延迟预算（Control Frequency & Latency Budget）

- **高频控制（例如 10-20Hz）**：
  - To=2 刚好覆盖 100-200ms 历史，足够捕捉短期运动趋势。
  - To=4 会增加 1 倍延迟（特征提取 + encoder + head），可能超出实时预算。

- **低频控制（例如 1-5Hz）**：
  - To=4 甚至 To=8 可行，因为单步允许更长计算时间。

### 2.3 数据分布与过拟合风险（Data Distribution & Overfitting）

- **To 越大**：
  - 优点：能捕捉更长时间依赖（比如"先抓、再移、再放"的多阶段任务）。
  - 缺点：更容易记住训练轨迹的具体顺序，泛化性下降；推理时若前 To 帧偏离训练分布，后续动作会崩溃。

- **To 越小**：
  - 优点：对单帧扰动更鲁棒，泛化更好。
  - 缺点：无法利用长期依赖（比如"记住刚才抓的是哪个物体"）。

---

## 3. 实际建议（按任务类型）

### 3.1 桌面单臂/双臂操作（你的当前场景）

**推荐 To=2**：

- 你的任务大多是"固定场景 + 一次性抓放"，动态性不高。
- To=2 能给 DP head 提供简单的速度信号（当前帧与前一帧的差），足够区分"静止/移动/抓取"等状态。
- 延迟可控：假设单帧特征提取 50ms，To=2 只需要 100ms 历史数据，不会成为瓶颈。

**如果你发现 To=2 性能不够好**，可以往两个方向调：
- **降到 To=1**（如果任务足够简单、环境几乎静态）
- **升到 To=4**（如果任务有明显的"摆动/跟踪"阶段，比如 `shake_bottle_horizontally`）

### 3.2 动态物体跟踪/复杂多阶段任务

**推荐 To=4**：

- 例如：`shake_bottle`（需要跟踪物体晃动）、`handover_*`（需要感知对方臂运动）。
- To=4 能覆盖 ~400ms 历史（假设 10Hz），足够捕捉一个完整的"抓-移-放"子动作。

### 3.3 极简/快速响应场景

**推荐 To=1**：

- 例如：`button_press`、`drawer_open`（一步到位，无需历史）。
- 最低延迟，适合高频控制（20-30Hz）。

---

## 4. 与对齐训练的 8 帧关系

再次强调：**对齐模块训练时的 8 帧切片 ≠ DP 推理时的 To**。

- 对齐训练的目的是让 encoder 学会"把 RGB 特征投影到 PC 空间"，它需要多样化的切片长度（1/2/4/8）来增强泛化。
- DP 推理时的 To 是**你决策所需的历史窗口长度**，完全独立选择。

如果你担心"对齐用 8 帧训练，DP 推理用 To=2 会不会不匹配"：
- **不会**。对齐 encoder 的输入只是 `[B, To, 4, 2048]`，To 在训练时见过 8，推理时用 2 一样能正常 forward（因为 encoder 是对每个 time step 独立处理 + fusion，不强制要求固定长度）。

---

## 5. 实验建议

如果你不确定 To 该选多少：

1. **先用 To=2 训练一个 baseline**（最常用、最稳）。
2. **再试 To=1**（如果任务简单）和 **To=4**（如果任务复杂），对比成功率。
3. **可视化对比**：记录推理时的动作序列，看 To=1 是否"抖动/反应慢"、To=4 是否"过度拟合/泛化差"。

---

## 6. 总结

| 问题 | 答案 |
|-----|------|
| **默认推荐 To** | **2**（覆盖大多数桌面操作） |
| **To 与对齐训练 8 帧关系** | **无强制依赖**（对齐训练是增强策略，DP 推理独立选择） |
| **如何调优 To** | 从 2 开始，任务简单降到 1，任务动态性强升到 4 |
| **To 越大越好吗** | **不是**（容易过拟合、延迟高） |

---

**最终建议：你先保持 To=2（现在的默认值），把闭环跑通；如果发现某些动态任务（比如 `shake_bottle`）性能不好，再针对性地试 To=4。**
